import numpy as np
np.random.seed(1234)
from operator import truediv

class nn_from_scratch:

  X_train = []
  Y_train = []
  X_validation = []
  Y_validation = []
  X_test = []

  in_dim = 784
  out_dim = 10
  W = []
  W_zeros = []
  delta_W_memory = []
 
  previous_update = []
  previous_m = []
  previous_v = []
  vt_hat_sqrt=[]
  v_t_sqrt = []      #use this in rmsprop
  eps = 0.0001    
  beta_1 = 0.9
  beta_2 = 0.999
  beta = 0.9
  t = 0
  
  
  def __init__(self):

    self.No_hidden_layer = 2
    self.No_of_Neurons = [100,100]
    self.batch_size = 64
    self.epoch_num = 2
    self.init_type = 'random'
    self.activation_type = 'tanh'
    self.loss_type = 'cross_entropy'
    self.gradient_type = 'momentum'

    self.activation = []
    self.pre_activation = []
    self.eta = 0.01
    self.gamma = 0.5

################################################################################    
    
    #Initializing weights with given type
    if self.init_type == 'random':
      start = self.in_dim
      for i in range(self.No_hidden_layer):
        j = self.No_of_Neurons[i]
        self.W.append(np.random.randn(j,start+1))
        start = j
      self.W.append(np.random.randn(self.out_dim,j+1))
    elif self.init_type == 'xavier':
      self.W = []
    else:
      print('Initialisation type is not in list')

################################################################################
    
    # Initializing all temporary parameters to zero
    start = self.in_dim
    for i in range(self.No_hidden_layer):
      j = self.No_of_Neurons[i]
      self.W_zeros.append(np.zeros((j,start+1)))
      start = j
    self.W_zeros.append(np.zeros((self.out_dim,j+1)))


    self.previous_update = np.copy(self.W_zeros)
    self.previous_m = np.copy(self.W_zeros)
    self.previous_v = np.copy(self.W_zeros)       

################################################################################
 
  #Forward_propogation algorithm returning output probability vector
  def FFN_return_output(self, input):
    h = (input.T).copy()     
    self.activation = []        
    self.pre_activation = []

    for i in range(self.No_hidden_layer):
      a = np.dot(self.W[i],np.vstack((h,1)))
      self.pre_activation.append(a)
      h = self.activation_function(a)
      self.activation.append(h)
      h = self.activation[-1]
      del a

    a = np.dot(self.W[-1],np.vstack((h,1)))
    self.pre_activation.append(a)
    self.activation.append(self.softmax(a))
    return self.activation[-1]

################################################################################
  
  # Computing labels for given input
  def predict_model(self,input):
    label_pred = []
    for i in range(input.shape[0]):
      Y_pred = self.FFN_return_output(np.array([input[i]]))
      label_pred.append(np.argmax(Y_pred))
    return label_pred

################################################################################

  # Computing Loss          
  def loss_calculation(self, input,ground_truth):

    loss = []
    label = []
    for i in range(input.shape[0]) :
      Y_pred = self.FFN_return_output(np.array([input[i]]))
      label.append(np.argmax(Y_pred))
      if self.loss_type == 'cross_entropy':
        loss.append(np.multiply(np.log(np.add(Y_pred.T,self.eps)).dot(np.array([ground_truth[i]]).T), (-1)))                     
      elif self.loss_type == 'square_error':
        loss.append(np.sum((Y_pred-ground_truth[i].T)**2)*.5)
      else:
        print("Wrong loss function entered")
    return np.mean(loss)
